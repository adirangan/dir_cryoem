run('/data/rangan/dir_cryoem/dir_rangan_playroom/setup_access1');
cd('/data/rangan/dir_cryoem/dir_CryoBIFE_MD');

flag_disp = 1; nf=0;
%%%%%%%%;
% Here we assume that LL_rc__(1+nr,1+nc) is the (affinely-transformed) log-likelihood: ;
% LL_rc__(1+nr,1+nc) = LL_a * log(P( A_{r} | F_{c} )) + LL_b ;
% for image A_{r} and model F_{c}. ;
% The affine-transformation is defined via the parameters LL_a and LL_b, ;
% which we assume are fixed for all images (nr) and models (nc). ;
%%%%%%%%;
LL_rc__ = load('-ascii','../dir_CryoBIFE_MD/LogLikeMat_20250129.mat'); %<-- log-likelihoods.; 
[n_r,n_c] = size(LL_rc__);

%%%%%%%%;
% With this assumption we can see that the individual likelihoods are given by: ;
% P_rc__ := P( A_{r} | F_{c} ) = exp((LL_rc__(1+nr,1+nc)-LL_b)/LL_a). ;
%%%%;
% for now we will pick an arbitrary LL_b and LL_a for exposition. ;
%%%%%%%%;
LL_b = 1.15*max(LL_rc__,[],'all'); %<-- arbitrary. ;
LL_a = max(1e-12,1024*std(LL_rc__,1,'all')); %<-- arbitrary. ;
P_rc__ = exp((LL_rc__-LL_b)./max(1e-12,LL_a));
%%%%;
%figure(1+nf);nf=nf+1;clf;figsml;
%hist(P_rc__(:),linspace(0,1,1024));
%xlim([0,1]);xlabel('P');
%ylabel('#');
%title('histogram of probabilities');

%%%%%%%%;
% We will further assume that the models are distributed via rho_c_, ;
% such that the probability of sampling model nc is rho_c_(1+nc). ;
% Thus, the marginalized likelihoods for each image are given by: ;
% P( A_{r} | {F_{c}} ) := P( A_{r} ) = \sum_{c} P(A_{r}|F_{c}) \rho_{c}, ;
% or P_r_ := sum( P_rc__*rho_c_ , 2 ) . ;
% Similarly, the 'total' likelihood of observing all the images would be: ;
% P( {A_{r}} ) = \prod_{r} P(A_{r}), ;
% or P := prod( P_r_ , 'all') ;
%%%%;
% Now, if we were to try and maximize P with respect to rho_c_, ;
% we will inevitably have to deal with the constraints: 
% G(rho_c_) = sum(rho_c_) = 1. ;
% and rho_c_(1+nc)>=0 for each nc. ;
% These are inconvenient, so instead let us define: ;
% eta_c_ --> rho_c_(1+nc) = exp(eta_c_(1+nc))/Z, ;
% where Z = sum(exp(eta_c_)).;
% This representation is not perfect (as there can be underflow yielding Z\approx 0), ;
% but it should be a reasonable start. ;
% Note that:
% \partial_{\eta_{d}} Z = \exp(eta_{d}). ;
% \partial_{\eta_{d}} \rho_{c} = \exp(\eta_{c})/Z*\delta_{dc} - \exp(\eta_{c})\exp(\eta_{d})/Z^{2} ;
%%%%;
% With this representation we have: ;
% P(A_{r}) = \sum_{c} P(A_{r}|F_{c}) * \rho_{c} = \sum_{c} P(A_{r}|F_{c}) * exp(\eta_{c})/Z. ;
% and: ;
% \partial_{\eta_{d}} log(P(A_{r})) = \frac{ \sum_{c} P(A_{r}|F_{c}) * \partial_{\eta_{d}} \rho_{c} }{P(A_{r})}
% which could be tackled using gradient-descent. ;
%%%%%%%%;

%%%%%%%%;
% For now, let us not worry about gradient-descent, and instead try and visualize the problem.
% To start, let us limit ourselves to ony two models. ;
%%%%%%%%;
nc0 = 0; nc1 = 1; %<-- pick two models. ;
elim_ = [-5,+5];
n_eta = 1024;
eta0_e_ = transpose(linspace(min(elim_),max(elim_),n_eta));
eta1 = 0;
Z_e_ = exp(eta0_e_) + exp(eta1);
rho0_e_ = exp(eta0_e_)./Z_e_;
rho1_e_ = exp(eta1)./Z_e_;
rho_ec__ = cat(2,rho0_e_,rho1_e_);
rho_ce__ = permute(rho_ec__,[2,1]);
%%%%%%%%;
% As a sanity check, imagine a scenario where the first 200 images are 9-times more likely to come from model 0, ;
% while the next 100 images are 9-times more likely to come from model 1. ;
%%%%%%%%;
tmp_P_rc__ = 0.1+[0.9*ones(200,1) , 0.0*ones(200,1) ; 0.0*ones(100,1) , 0.9*ones(100,1) ];
logP_e_ = reshape(sum( log( sum(bsxfun(@times,tmp_P_rc__(:,1+[nc0,nc1]),reshape(rho_ce__,[1,2,n_eta])),[2]) ) , [1] ),[n_eta,1]);
%%%%;
figure(1+nf);nf=nf+1;clf;figsml;
fontsize_use = 12;
plot(eta0_e_,logP_e_,'k.-');
xlim(elim_); xlabel('$\eta_{0}$','Interpreter','latex');
ylabel('$\log(P)$','Interpreter','latex');
set(gca,'FontSize',fontsize_use);
%%%%%%%%;
% As you can see, the optimal \log(P) occurs when \rho_{c} \approx \{ 2/3 , 1/3 \}. ;
%%%%%%%%;
[~,tmp_ij] = max(logP_e_);
rho0_opt = rho0_e_(tmp_ij);
rho1_opt = rho1_e_(tmp_ij);
disp(sprintf(' %% sanity-check: (rho0_opt,rho1_opt) = (%+0.3f,%+0.3f)',rho0_opt,rho1_opt));
%%%%%%%%;
% Now run the same code using two of the models from the data-set. ;
%%%%%%%%;
nc0 = 2; nc1 = 3; %<-- pick two models. ;
logP_e_ = reshape(sum( log( sum(bsxfun(@times,P_rc__(:,1+[nc0,nc1]),reshape(rho_ce__,[1,2,n_eta])),[2]) ) , [1] ),[n_eta,1]);
%%%%;
figure(1+nf);nf=nf+1;clf;figsml;
fontsize_use = 12;
plot(eta0_e_,logP_e_,'k.-');
xlim(elim_); xlabel('$\eta_{0}$','Interpreter','latex');
ylabel('$\log(P)$','Interpreter','latex');
set(gca,'FontSize',fontsize_use);
%%%%%%%%;
[~,tmp_ij] = max(logP_e_);
rho0_opt = rho0_e_(tmp_ij);
rho1_opt = rho1_e_(tmp_ij);
disp(sprintf(' %% first-two-models in real-data: (rho0_opt,rho1_opt) = (%+0.3f,%+0.3f)',rho0_opt,rho1_opt));

%%%%%%%%;
% Now we could also work on three models. ;
%%%%%%%%;
nc0 = 0; nc1 = 1; nc2 = 2; %<-- pick three models. ;
elim_ = [-5,+5];
n_eta = 128;
eta0_e_ = transpose(linspace(min(elim_),max(elim_),n_eta));
eta1_e_ = transpose(linspace(min(elim_),max(elim_),n_eta));
eta2 = 0;
[eta0_ee__,eta1_ee__] = ndgrid(eta0_e_,eta1_e_);
Z_ee__ = exp(eta0_ee__) + exp(eta1_ee__) + exp(eta2);
rho0_ee__ = exp(eta0_ee__)./Z_ee__;
rho1_ee__ = exp(eta1_ee__)./Z_ee__;
rho2_ee__ = exp(eta2     )./Z_ee__;
rho_eec___ = cat(3,rho0_ee__,rho1_ee__,rho2_ee__);
rho_cee___ = permute(rho_eec___,[3,1,2]);
%%%%%%%%;
% As a sanity check, imagine a scenario where the first 300 images are most likely to come from model 0, ;
% while the next 200 images are most likely to come from model 1, ;
% and the next 100 images are most likely to come from model 2, ;
%%%%%%%%;
tmp_P_rc__ = [ ...
  0.8*ones(300,1) , 0.1*ones(300,1) , 0.1*ones(300,1) ...
; 0.1*ones(200,1) , 0.8*ones(200,1) , 0.1*ones(200,1) ...
; 0.1*ones(100,1) , 0.1*ones(100,1) , 0.8*ones(100,1) ...
];
logP_ee__ = reshape(sum( log( sum(bsxfun(@times,tmp_P_rc__(:,1+[nc0,nc1,nc2]),reshape(rho_cee___,[1,3,n_eta,n_eta])),[2]) ) , [1] ),[n_eta,n_eta]);
%%%%;
figure(1+nf);nf=nf+1;clf;figsml;figbeach;
fontsize_use = 12;
imagesc(logP_ee__);
axisnotick();
ylabel('$\eta_{0}$','Interpreter','latex');
xlabel('$\eta_{1}$','Interpreter','latex');
set(gca,'FontSize',fontsize_use);
%%%%%%%%;
% As you can see, the optimal \log(P) occurs when \rho_{c} \approx \{ 3/6 , 2/6 , 1/6 \}. ;
%%%%%%%%;
[~,tmp_ij] = max(logP_ee__,[],'all');
rho0_opt = rho0_ee__(tmp_ij);
rho1_opt = rho1_ee__(tmp_ij);
rho2_opt = rho2_ee__(tmp_ij);
disp(sprintf(' %% sanity-check: (rho0_opt,rho1_opt,rho2_opt) = (%+0.3f,%+0.3f,%+0.3f)',rho0_opt,rho1_opt,rho2_opt));
%%%%%%%%;
% Now run the same code using three of the models from the data-set. ;
%%%%%%%%;
nc0 = 0; nc1 = 1; nc2 = 2; %<-- pick three models. ;
logP_ee__ = reshape(sum( log( sum(bsxfun(@times,P_rc__(:,1+[nc0,nc1,nc2]),reshape(rho_cee___,[1,3,n_eta,n_eta])),[2]) ) , [1] ),[n_eta,n_eta]);
%%%%;
figure(1+nf);nf=nf+1;clf;figsml;figbeach;
fontsize_use = 12;
imagesc(logP_ee__);
axisnotick();
ylabel('$\eta_{0}$','Interpreter','latex');
xlabel('$\eta_{1}$','Interpreter','latex');
set(gca,'FontSize',fontsize_use);
%%%%%%%%;
% Now optimize: ;
%%%%%%%%;
n_3 = 3;
%nlP3 = @(eta_c_) -sum( log( P_rc__(:,1+[nc0,nc1,nc2])* exp(eta_c_)/max(1e-300,sum(exp(eta_c_))) ) , 1 );
nlP3 = @(eta_ct__) -sum( log( bsxfun( @rdivide , P_rc__(:,1+[nc0,nc1,nc2])* exp(eta_ct__) , max(1e-300,sum(exp(eta_ct__),1)) ) ) , 1 );
tmp_option = optimset('Display','iter','MaxFunEvals',1024,'MaxIter',1024,'TolFun',1e1);
[eta_opt_c_,nlP_opt] = fminsearch(nlP3,zeros(n_3,1),tmp_option);
rho_opt_c_ = exp(eta_opt_c_)/max(1e-300,sum(exp(eta_opt_c_)));
disp(sprintf(' %% (nc0,nc1,nc2)=(%.2d,%.2d,%.2d) --> rho_opt_c_=(+%0.3f,+%0.3f,+%0.3f)',nc0,nc1,nc2,rho_opt_c_));

%%%%%%%%;
% Now we can apply the same idea across the entire data-set. ;
%%%%%%%%;
LL_b = 1.15*max(LL_rc__,[],'all'); %<-- arbitrary. ;
LL_a = max(1e-12,1024*std(LL_rc__,1,'all')); %<-- arbitrary. ;
P_rc__ = exp((LL_rc__-LL_b)./max(1e-12,LL_a));
P_rc__ = bsxfun(@rdivide,P_rc__,sum(P_rc__,1));
P_rc__ = bsxfun(@rdivide,P_rc__,sum(P_rc__,2));
P_rc__ = bsxfun(@rdivide,P_rc__,sum(P_rc__,1));
P_rc__ = bsxfun(@rdivide,P_rc__,sum(P_rc__,2));
%%%%%%%%;
tmp_option = optimset('Display','off','MaxFunEvals',1024,'MaxIter',1024,'TolFun',1e1);
nlPX = @(eta_ct__) -sum( log( bsxfun( @rdivide , P_rc__* exp(eta_ct__) , max(1e-300,sum(exp(eta_ct__),1)) ) ) , 1 );
n_t = 1024*32;
rng(0);
eta_rand_ct__ = max(elim_)*randn(n_c,n_t);
nlP_t_ = nlPX(eta_rand_ct__);
[~,tmp_ij_] = sort(nlP_t_,'ascend');
n_s = min(n_t,128);
nlP_opt = +Inf; eta_opt_c_ = zeros(n_c,1);
for ns=0;n_s-1;
tmp_ij = tmp_ij_(1+ns);
eta_init_c_ = eta_rand_ct__(:,tmp_ij);
[eta_sub_c_,nlP_sub] = fminsearch(nlPX,eta_init_c_,tmp_option);
if (nlP_sub< nlP_opt);
nlP_opt = nlP_sub;
eta_opt_c_ = eta_sub_c_;
end;%if (nlP_sub< nlP_opt);
end;%for ns=0;n_s-1;
rho_opt_c_ = exp(eta_opt_c_)/max(1e-300,sum(exp(eta_opt_c_)));
%%%%;
figure(1+nf);nf=nf+1;clf;figmed;
markersize_use = 16;
subplot(1,2,1);
plot(1:n_c,eta_opt_c_,'g.','MarkerSize',markersize_use);
xlim([0,1+n_c]); xlabel('model-ij 1+nc');
ylabel('eta');
subplot(1,2,2);
plot(1:n_c,rho_opt_c_,'r.','MarkerSize',markersize_use);
xlim([0,1+n_c]); xlabel('model-ij 1+nc');
ylabel('rho');
%%%%%%%%;
% As you can see, the high overall likelihood of model nc==4 dominates the calculation. ;
% This, coupled with the fact that the model nc==4 is not relatively unlikely for many images, ;
% produces a maximum-likelihood estimate for rho_c_ that is dominated by rho_c_(1+4). ;
%%%%%%%%;
figure(1+nf);nf=nf+1;clf;figsml;
hold on;
plot(1:n_c,mean(P_rc__,1),'o');
plot(1+4,mean(P_rc__(:,1+4),1),'rx');
hold off;
xlim([0,n_c+1]); xlabel('model-ij 1+nc');
grid on;
%%%%%%%%;

%%%%%%%%;
% Going forward I will try and think about moving away from maximum-likelihood estimates, ;
% (e.g., accounting for the volume of the phase-space corresponding to rho_c_). ;
%%%%%%%%;


